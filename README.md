# NLP-task-classification-of-texts-by-topic
# Decription
## Часть 1
Давайте создадим функцию, которая будет предобрабатывать наш текст.
(При считывании файла я обнаружил кодинг xml документа, избавляюсь от него тоже)
После применения функции, давайте посмотрим на стоп-слова которые могут находиться в нашем тексте.

При лемматизации изначально выбор пал на библиотеку Natasha, но т.к после 16 часового ожидания не удалось достичь результата, я воспользовался библиотекой от nltk.
Далее я токенезирую текст и удаляю оттуда стоп-слова. 
Создаю очищенный фрейм.

Смотрю на баланс классов - класс "Мир" выделяется на фоне остальных. Поэтому в дальнейшем я буду искусственно понижать размерность train части и повышать часть test.
Это сбалансирует классы
## Часть 2
Давайте создадим pipeline с использованием tF-iDF-SGD and tF-iDF-LogReg моделей, посмотрим как они справятся со своей задачей.
(Присутствовал pipeline моделей tF-iDF-kNN, но т.к результат был не совсем хороший, я исключил из поля зрения)

И наконец-таки 
Производительность наших pipeline'ов


**Показатели обучения, такие как точность precision, recall, and f1-score**

Pipeline моделей tF-iDF-LogReg показал хороший результат.
(Но т.к мне не удалось переиграть Grid, я думаю, что SGD с гипер-параметрами покажет результат ничем не хуже)
## Часть 3
Изначально я использовал только текст новостей и таргет. В новостной сводке находилось достаточно информации для того, чтобы наши классификаторы умели правильно определять класс.
Проверить свою гипотезу я хочу проверить на LSTM модели.

Давайте применим функции чтобы предобработать наш титл.

Обычно при токенизации и сопоставлении целых чисел словам принцип такой: наиболее популярным словам соответствуют меньшие индексы (но ноль зарезервирован – об этом ниже).

Сопоставляем каждому слову в словаре целое число – этот шаг был успешным и эти данные пойдут в нейросеть.

Все титлы состоят из разного числа слов: для удобства можно подсчитать длину всех титлов:

Есть и длинные заголовки, возмножно с ними стоит поработать отдельно, чтобы посмотреть, какую часть можно убрать из объекта.

Делаем padding, если длинна меньше seq_len, если больше – берем первые seq_len индексов

Train_test_split

Для того, чтобы отдать наши данные LSTM нам необходимо воспользоваться Tensor'ом и Loader'ом.
Мы создали лист классов и установили max_words на 7 , чтобы сообщить, что мы хотим сохранить максимум 7 токенов на титл примере.

Далее мы определяем нашу сеть LSTM , которая состоит из 3-ех слоев.
1. Embedding Layer - принимает входной список индексов, сгенерированных функцией векторизации
2. LSTM Layer - Принимает вложения, сгенерированные слоем Embedding Layer, в качестве входных данных
3. Linear Layer - Имеет 6 единиц вывода, которые совпадают с количеством целевых классов. 
(Он принимает последний вход слоя LSTM и возвращает прогноз)

Давайте проверим нашу архитектуру, вызывая класс объекта.

Далее трейн нетворк

Функция принимает model(LSTM), loss function, optimizer, train data loader, validation data loader и количество эпох в качестве входных данных. 
Затем он выполняет обучающий цикл. Для каждой эпохи он перебирает все обучающие данные в пакетах, используя train data loader. 
Для каждого пакета данных он выполняет прямой проход, чтобы делать прогноз, вычислять потери, вычислять градиенты и обновлять веса сети.

Ниже мы обучаем нашу сеть. Мы инициализировали количество эпох до 13 и скорость обучения до 0,001 . 
Затем мы инициализировали функцию потери перекрестной энтропии, наш текстовый классификатор LSTM и оптимизатор Адам.

И наконец-таки 
Производительность нашей сети


**Показатели обучения, такие как точность precision, recall, and f1-score**
